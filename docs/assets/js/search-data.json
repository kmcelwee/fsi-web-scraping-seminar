{"0": {
    "doc": "Intro to APIs",
    "title": "Intro to APIs",
    "content": "# Introduction to working with APIs An API (\"Application Programming Interface\", you don't need to remember that, no one does.) is a collection of commands you can issue over the internet to collect data from websites. Collecting this data is usually done over code, but you can also see this data by just visiting URLs in your browser. This URL [https://api.github.com/users/princeton-cdh/repos](https://api.github.com/users/princeton-cdh/repos) will list information about all the code under the CDH's GitHub account. (If you're in Firefox, click on the \"Raw Data\" button to see things more clearly.) This is JSON data like any other, and you could write a quick script to list out all the codebases we've worked on. The power of this is that as long as you follow the formula above: ``` https://api.github.com/users/{USER}/repos ``` you can get the public code bases on *any* GitHub user. From that information we could then get a count of all issues on all the repositories so that we know how much work is left to do. GitHub is pretty complicated, but here's a link to the [API documentation](https://docs.github.com/en/rest/guides/getting-started-with-the-rest-api). If we didn't have an API, to accomplish something similar we'd have to scrape the CDH's [profile page](https://github.com/Princeton-CDH), which would be *a lot* harder. APIs store information in a clean, consistent way that makes it easy to collect information (for better and for worse.) ## Wrappers We will be working with `tweepy` a library that helps us to work with the API in python. This is an example of a **wrapper**. APIs are built to work with any language— all you need is an internet connection. But as we'll see, it can be clumsy to work directly with the API, so programmers called API wrappers like `tweepy ` to make their code cleaner and easier to use. If you're ever looking to work with an API, let's take Reddit as an example, it's worth googling \"reddit api python wrapper\", and you'll come across libraries that other people have used to connect to Reddit's API. For python, I recommend [`praw`](https://github.com/praw-dev/praw). Wrappers are simply supposed to simply make your life easier, but it's possible that you're going to work with APIs that don't have wrappers in the language you need. The New York Times API, for example, doesn't have a good python wrapper. If you make one, publish it online, and you can help other programers! ## API Limitations Rate limits are an important concept in order to work safely with APIs. When companies like Spotify or Twitter open up their servers to connect with other computers, they want to impose rules to make sure no one spams their servers or scrapes their entire database too quickly. When considering how to pursue a research question, keep \"rate limiting\" and \"data limiting\" in mind. ### Rate limiting Rate limits are kind of like speed limits imposed by an API. If Twitter allowed anybody to scrape every single tweet ever, it would place immense strain on their infrastructure (and since data is their main commodity, they can sell this access to corporations for a hefty price.) Rate limits [are documented here.](https://developer.twitter.com/en/docs/twitter-api/rate-limits) We have created one app, so in each 15-minute window we can execute 300 searches. ![](img/rate-limiting.png) Since each search contains a maximum of 200 tweets, we can get 60,000 tweets (!) in 15 minutes. When just starting out, rarely will one's research questions reach the limit, but if you need a lot of data for your analysis, your scrape may take days to execute if the API's rate limit is especially low. Smaller companies probably will have stronger limitations on how much data you can collect from their site. In the case of Facebook and TikTok, you can't scrape data at all. ### Data Limiting As discussed previously, this data is highly valuable, and companies like Facebook have decided to cut off developers from accessing that data. Spotify won't allow programmers to access counts of how long a song has been listened to (even though it's available in their UI.) Twitter limits users to only the latest 3200 tweets when scraping a user's timeline. You can only get the latest 100 retweets on a tweet [which has led to problems in the past](https://levels.io/giveaway/). When pursuing a research question, make sure that the resources that you need to answer that question willl be fully included in the dataset you're collecting [otherwise you may be misinforming people.](https://towardsdatascience.com/fake-follower-calculators-misinform-users-journalists-with-dubious-statistics-659b60fc4d5a) ## Secret Keys If someone has your public and private key, they can log in to the API and make it appear like you are abusing the rate limit or even post tweets as you (!). Never publish your private key anywhere. ## Legal **I AM NOT A LAWYER, THIS IS NOT LEGAL ADVICE** As long as you're using the API, you're not doing anything illegal, but web scraping in general (say if I wrote a script to scrape all the email addresses from https://www.princeton.edu/) is quasi-illegal. We won't go into how to scrape outside of APIs in these seminars, but consider that making thousands of requests to any website can cripple its infrastructure. This is called a [Denial of Service Attack (DoS).](https://www.paloaltonetworks.com/cyberpedia/what-is-a-denial-of-service-attack-dos) So it's best to follow [best practices](https://data-lessons.github.io/library-webscraping-DEPRECATED/05-conclusion/) when collecting data outside the formal API process. ## Ethics Working with Twitter data most users know that Twitter is a public place, and anyone can see their tweets. Even better, working with Reddit, people are operating under the assumption of anonymity, so there are fewer ethical considerations. But when you send a tweet, you may assume that it's public, but how much control should you have after posting to a platform? 🤔 **How would you feel if a tweet you wrote...** * ...was included as an example in a book about internet culture? * ...was used to train a machine learning model to detect sarcasm? * ...included a picture that was used to train a facial recognition algorithm? (All these are real-life examples.) As you develop your research question, consider the ethical implications of collecting this data. If you scraped a user's timeline, was this person a public figure? If you scraped all tweets that used a certain hashtag, how many people are you collecting data from? Did any of these people expect their data to be collected by a first years at Princeton? ### Popular APIs * [Wikipedia](https://www.mediawiki.org/wiki/API:Main_page) * [Reddit](https://www.reddit.com/dev/api) * [Spotify](https://developer.spotify.com/documentation/web-api/) * [more...](https://github.com/public-apis/public-apis) ## The Twitter API ### General structure Twitter is exceptional as a social media company because it gives researchers access to almost its entire site. Twitter allows you to access the following databases: * Tweets * Users * Direct Messages * Lists * Trends * Media * Places 📚 **Given these databases, what kind of questions could you ask?** ### What is a Twitter ID? A Twitter ID is a unique identifier for each tweet. Having a unique ID is important for Twitter to store the thousands of tweets posted every second. If you have the ID of a tweet (perhaps from JSON data that you've scraped) and you want to see the original tweet in your browser, just follow this recipe: ``` https://twitter.com/{PROFILE}/status/{TWEET_ID} # Example: https://twitter.com/mattxiv/status/1368246126302945284 ``` If you'd like, try to use this tweet ID `1116487177364365313` to find the original Tweet. Often, organizations like [Documenting the Now](https://www.docnow.io/) will store tweet IDs only, and then researchers can \"rehydrate\" those IDs. Since there's a limited time window to collect tweets during momentous occasions, you're welcome to contribute to resources like these to preserve how people reacted online. [Check out Documenting the Now's catalog.](https://catalog.docnow.io/) ## Existing datasets Before scraping your own datasets, consider that other people may have had similar questions as you. Check out these sites before beginning your work to see if other researchers can save you time! People who create datasets want them to be put to good use, so they'll try to share them as widely as possible. * Wikipedia has a lot of tabular data you can draw from! * [Github](https://github.com/) is often used for code, but researchers will often post their datasets there as well * [Documenting the Now](https://catalog.docnow.io/), as mentioned above, has a huge resource of tweets * [r/datasets](https://www.reddit.com/r/datasets) contains a lot of datasets that people compiled for their research or for fun. ",
    "url": "/seminar-1/apis",
    "relUrl": "/seminar-1/apis"
  },"1": {
    "doc": "Intro to Colab",
    "title": "Intro to Colab",
    "content": "# Introduction to Colab Google Colab is an online platform to write computer code in what's called a code \"notebook\". Notebooks allow programmers to combine charts, text, and code in one place. Typically, programmers write code in files on their computers and then run those programs on their own computer, but that requires some setup that can make things complicated. Similar to a Google Doc, Colab allows you to edit code right in the browser. To add text to your notebook, you'd be using a language called \"markdown\". Markdown is a popular syntax used to add formatting to plain text like bolding a word or adding links. You don't need to know too much about markdown, but it's nice to know the basics if you're interested. (This site was built using just markdown!) * [Learn more about markdown.](https://www.markdownguide.org/getting-started/) * [Learn more about markdown syntax.](https://www.markdownguide.org/basic-syntax/) ## Executing your first code block 1. Go to the [Colab website](https://colab.research.google.com), you should be greeted with an introductory notebook. (This intro notebook has some pretty complicated stuff, so don't get too worried about everything that's written there.) 2. Edit any code block and click the \"play\" button on the left side. A useful shortcut to remember is Shift + Enter. This will execute whatever code block you're currently editing. 3. Create a new notebook. `File > New Notebook` 4. In the upper left-hand corner, rename the notebook to \"first-notebook.ipynb\" 5. Copy the following code into a cell, and execute the cell: ```python print('Welcome to Colab!') ``` ## Example notebook Let's say we are interested in the Twitter account [@dog_feelings](https://twitter.com/dog_feelings/), and we want to know what day of the week the account tweets most frequently. I already have a CSV of the account's tweets prepared ([link](https://raw.githubusercontent.com/kmcelwee/fsi-web-scraping-seminar/main/data/dog_feelings-tweets.csv)). Create a markdown cell with the heading \"Analyze Twitter account [@dog_feelings](https://twitter.com/dog_feelings/)\". To add a text cell, look for the `+ Text` button at the top of your notebook. This is what that would look like in markdown: ```md # Analyze Twitter account [@dog_feelings](https://twitter.com/dog_feelings/) ``` Create a new code block (`+ Code` button), copy the following code block, and execute it. By the end of these two seminars, you should understand what this code does! ```python # Research question: What is the most popular day of week for @dog_feelings to tweet? # import a library that helps us parse CSV files import pandas as pd # Read the CSV from the provided link and place into the variable `df` (short for dataframe) df = pd.read_csv('https://raw.githubusercontent.com/kmcelwee/fsi-web-scraping-seminar/main/data/dog_feelings-tweets.csv') # We want to work with time data, but it takes an extra step to make sure that # the dataframe parsed the timestamp correctly. Here we use the to_datetime # function to turn the `string` into a `datetime` object, a common object in # python that allows us to easily get the day of the week. df['timestamp'] = pd.to_datetime(df['timestamp']) # Create a new column `day-of-the-week` using the timestamp column df['day-of-week'] = df['timestamp'].dt.dayofweek # Group by day of the week and plot the count as a bar chart, and give a title! df.groupby('day-of-week')['id'].count().plot(kind='bar', title='Number of tweets tweeted by @dog_feelings by day of week.') # Note: 0 is Monday and 6 is Sunday ``` View expected output Again, you don't understand every component here, but the benefit of Colab is that you can combine your notes, charts, and code all in one place. 📈 **Exercises:** 1. Given the chart created by the code above, what is the answer to our research question? 2. What are some questions that come up from this chart? How might it be improved? 3. The header of the CSV is `timestamp,id,text,favorite_count,retweet_count,hashtags`. What other questions could we answer using this data? ## Other helpful hints * It's easy to execute cells out of order. When you go back to your notebook it's possible errors might pop up if you execute them one after another. The number in brackets to the left of your code is the order of cell execution. If you have two code cells and one relies on the other, then you'll need to execute the first cell before executing the second. * By hovering over many of the buttons, you'll see keyboard shortcuts that are useful to remember and will save you a bit of time. Here are some worth remembering: * Shift + Enter: Execute the code in this cell * Command / Ctrl + M + B: Create a cell below this cell * Command / Ctrl + M + A: Create a cell above this cell * Command / Ctrl + M + D: Delete this cell * Command / Ctrl + M + Z: Undo * Don't forget to turn on [\"Corgi Mode\"](https://twitter.com/GoogleColab/status/1116487177364365313) * Check out how jupyter notebooks are used in [Nobel prize winning scientific research](https://github.com/jkanner/aapt/blob/master/AAPT-WM19-Romano.ipynb) ",
    "url": "/seminar-1/colab",
    "relUrl": "/seminar-1/colab"
  },"2": {
    "doc": "How to develop a research question",
    "title": "How to develop a research question",
    "content": "# How to develop a research question ## Typical research workflow 1. Be curious and find a question that interests you. 1. Try to find a way to answer your question with data. What kind of assumptions are you making when you make this leap? 1. What are all the fields you need to answer this data-driven question? 1. Execute scrape. 1. Pare down data into just the information you need. 1. Enrich the dataset with any external datasets if necessary. (If manually adding data, ensure that the time you're prepared to invest are worth the question you're trying to answer!) 1. Analyze your data! Build charts to answer your original question. Were you wrong? That's okay! That means you probably had a counter-intuitive result. What led you to have a wrong hypothesis? ## Case Study 2: Analyzing Congressional Policy What policy issues does Congresswoman Terri Sewell (D-AL 7th District, Princeton Class of 1986) care about? By scraping her account, we could collect all of her tweets and categorize them by issue [as The Pudding did.](https://congress.pudding.cool/person/RepTerriSewell) 🏛️ **Questions** * What kind of concrete takeaways can you make from Sewell's Twitter activity? * What kind of interpretive takeaways can you make from Sewell's Twitter activity? * What techniques did the Pudding use to put their data into perspective? * How effective is this answering our original question? ## Twitter Scraping Inspiration * Subreddits * [r/datasets](https://www.reddit.com/r/datasets/): Search here for your datasets before scraping! Sometimes someone else has already done the work. * [r/dataisbeautiful](https://www.reddit.com/r/dataisbeautiful/): Where data visualization fans show off their work. You'll often see the cutting-edge here. * [r/dataisugly](https://www.reddit.com/r/dataisugly/): Negative inspiration. * Data journalists * Data journalists are often the most approachable forms of data-driven research that you can read. * [The Pudding](https://pudding.cool/) * [Washington Post Graphics](https://twitter.com/PostGraphics?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) * [NYT Graphics](https://twitter.com/nytgraphics?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) * Data visualization projects and essays * [The Fortune 100 and Black Lives Matter](https://www.brownanalytics.com/fortune-100-blm-report/site/index.html): How did Fortune 100 companies react on Twitter to the death of George Floyd and the subsequent protests in the summer of 2020? * [What is Congress tweeting about?](https://congress.pudding.cool/): Topic modeling to monitor what Congress is most ![](img/blm-fortune-100.png) 🪚 **Exercise: Let's workshop on 2-3 potential research questions together.** ",
    "url": "/seminar-1/examples",
    "relUrl": "/seminar-1/examples"
  },"3": {
    "doc": "Home",
    "title": "Home",
    "content": "# Introduction to Web Scraping with Python A collection of web scraping resources for the 2021 FSI summer course [Humanistic Approaches to Media and Data](https://sifp.princeton.edu/humcf). If you need to contact me or learn more about my path into becoming a programmer check out my [profile on the Center for Digital Humanities website](https://cdh.princeton.edu/people/kevin-mcelwee/). ## Seminar Goals Goal of [Seminar 1](seminar-1/index) (3 hours): * [Review basics of programming with Python.](seminar-1/python) * [Introduce Google Colab.](seminar-1/colab) * [Understand data types and file formats (CSV & JSON).](seminar-1/json-and-csv) * [Log into the Twitter API and execute our first query.](seminar-1/tweepy) * [Be familiar with rules and limitations of working with APIs.](seminar-1/apis) * [Trim raw JSON files into a useable CSV file.](seminar-1/munge-json) * [Talk about potential projects / get inspiration](seminar-1/examples) Goal of Seminar 2 (3 hours): * Understand basics of manipulating tabular data with Pandas. * Draw inspiration from accessible research driven by APIs. * Generate basic plots with data collected from the previous. ## Tools We will be working exclusively with Google Colab. In order to interact with the Twitter API, we will be using the Python wrapper [`tweepy`](https://docs.tweepy.org/en/stable/). ## Prerequisites The seminars expect that you already have an approved Twitter Developer Acccount. [Apply for access to the Twitter API](https://developer.twitter.com/en/apply-for-access). A basic understanding of Python is also expected. ## Coding Resources * [YouTube Python Tutorials](https://www.youtube.com/results?search_query=python+tutorials) * [O'Reilly Learn](https://learning.oreilly.com/home/): A collection of every programming book you'll ever need. A subscription is free with your netID and offers video lectures, introductory books, and answers to [frequently asked programming questions.](https://learning.oreilly.com/answers/search/) * [Free programming books](https://github.com/EbookFoundation/free-programming-books/blob/master/books/free-programming-books.md#python): Tons of programming materials are free online. It doesn't matter too much how you get started, as long as you just start! ## Useful Links * [Twitter Documentation](https://developer.twitter.com/en/docs) * [Tweepy Documentation](https://docs.tweepy.org/en/stable/) * [StackOverflow](https://stackoverflow.com/) * [Google Colaboratory](https://colab.research.google.com) * [YouTube Python Tutorials](https://www.youtube.com/results?search_query=python+tutorials) * [Practice coding Python online](https://www.hackerrank.com/domains/python) * [YouTube Pandas Tutorials](https://www.youtube.com/results?search_query=pandas+tutorials) * [Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) * [Coursera Python Web Data course](https://www.coursera.org/learn/python-network-data) ## Vocabulary * Web scraping: Collecting data from the internet in an automated way * API: Application Programming Interface. In the context of web scraping, it is a system used by web site owners to monitor and control how data exits their platform. * HTTP Methods: (e.g. POST, GET, UPDATE, DELETE) For web scraping we're only interested in what are called \"GET requests\", a request made to the website's server for information. With that request, you include the type of information you need, and usually an authorization token. * Rate limiting: The speed limit placed on programmers that prevents them from making too many requests at once and overworking a site's servers. This varies from site to site. * JSON: JavaScript Object Notation. A lightweight data format used throughout the web. If you receive a response through an API, it will almost certainly be in this format. * Wrapper: In web scraping a wrapper is a library of code that translates the API into a language that you're comfortable programming in. * Python: The language most often used for writing quick scripts for web scraping and data analysis. If you are using an API, there's a good chance that there's a wrapper in Python. Python has also been embraced by the data science community and has many libraries to support data cleaning and visualization. ## Thank you Thank you to [Kavita Kulkarni](https://cdh.princeton.edu/people/kavita-kulkarni/) for her guidance in constructing these seminars. And thank you to Melanie Walsh for basically [creating this course first](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) 😄 ",
    "url": "/",
    "relUrl": "/"
  },"4": {
    "doc": "Seminar 1",
    "title": "Seminar 1",
    "content": "# Seminar 1 **July 16, 2021 (1:30-4pm)** By the end of this seminar, I hope that you'll be able to request data from APIs, specifically Twitter, transform data between data formats, and develop research questions. We'll also explore the limitations of data-driven research in answering those research questions. In the next seminar, we'll learn different ways to analyze data once we have it in a format we like. ",
    "url": "/seminar-1",
    "relUrl": "/seminar-1"
  },"5": {
    "doc": "File formats",
    "title": "File formats",
    "content": "# Working with different file formats ## What is a CSV? If you've ever worked with a spreadsheet, you know what a CSV is. A CSV is just a standardized way of storing data in a table. CSV stands for \"Comma Separated Value\", meaning literally each value in row is separated by a comma. Another popular format is TSV or \"Tab Separated Value\". It's the same as a CSV, but just separates values by tabs. This is especially useful when there are a lot of commas in your data. Here's what a CSV looks like, (Fortune 100 companies and their Twitter handles): ``` Corporation,URL,Rank,Handle,Sector Walmart,https://twitter.com/Walmart,1,Walmart,Retailing Amazon.com,https://twitter.com/amazon,2,amazon,Retailing Exxon Mobil,https://twitter.com/exxonmobil,3,exxonmobil,Energy ``` That data when loaded into Excel or Google Sheets would look like this: | Corporation | URL | Rank | Handle | Sector |-------------|--------------------------------|------|------------|-----------| Walmart | https://twitter.com/Walmart | 1 | Walmart | Retailing | Amazon.com | https://twitter.com/amazon | 2 | amazon | Retailing | Exxon Mobil | https://twitter.com/exxonmobil | 3 | exxonmobil | Energy | CSVs are important because they are non-priorietary and plain text. Therefore they can easily be parsed and created by the software programs we write. We'll get explain how to parse CSVs when we learn about the [Pandas library next week.](../seminar-2/introduction-to-pandas) One drawback of CSVs is that they are limited to one value for each row and each column. If we were trying to add a `Hashtags Used` column, we might get ten or fifteen values in each row. If, for example, Walmart used the hashtags \"#retail\", \"#IndependenceDay\", and \"#savings\", we could combine with some kind of unique character like a semicolon: `retail;IndependenceDay;savings`. Or we could use JSON... ## What is JSON? Whenever scraping the web, you will inevitably get a JSON (often pronounced like Jason) response. It's arguably the most popular data format on the web. JSON stands for \"JavaScript Object Notation\", but you really don't need to remember this. It's a combination of nested lists and dictionaries, the same kind of lists and dictionaries that you'd use in python. View an example tweet JSON from Twitter As you can see, there's *a lot* of information for just a single tweet, and representing that information in a CSV would be nearly impossible. JSON makes it easy to communicate complex datasets in a plain text format. ### Parsing JSON in Python Again, JSON is just a combination of dictionaries and lists. To open a JSON file and get certain values, you'd treat it as you would any list or dictionary. You can load a JSON file using Python's `json` library: ```python import json json_file_path = 'tweet.json' with open(json_file_path) as f: tweet = json.load(json_file_path) ``` Feel free to copy the following into a Colab notebook and mess around with the `tweet` variable. Here are some examples of getting certain values: ```python import json import requests r = requests.get('https://raw.githubusercontent.com/kmcelwee/fsi-web-scraping-seminar/main/data/tweet.json') tweet = json.loads(r.text) # To get the text of this tweet tweet['full_text'] # To get the users mentioned in this tweet tweet['entities']['user_mentions'][0]['screen_name'] ``` It can sometimes be difficult to figure out the path to a variable in JSON, so some trial and error is predictable. 💡 **Exercise: Can you make a list of the hashtags with the example tweet provided above?** View Solution ",
    "url": "/seminar-1/json-and-csv",
    "relUrl": "/seminar-1/json-and-csv"
  },"6": {
    "doc": "Data cleaning",
    "title": "Data cleaning",
    "content": "# Data Cleaning The most important (and least glamorous) part of working with data is creating a data pipeline. This data pipeline implements a cleaning process where you transform the data you find in the wild into a dataset that you can use to answer your research question. Data cleaning includes... * Joinining multiple datasets together. * Updating values to adhere to a standard. * e.g. survey data may ask for class year and you'll get \"2024\", \"rising sophomore\", \"senior\", etc., and your goal would be to translate them all into four categories: 2022, 2023, 2024, 2025 * Manually splitting up data into categories or sorting data. * Removing unnecessary data. * Changing file formats to fit technical requirements. This is also sometimes called \"data munging\", and is often the longest part of the research process. Usually when drawing data from an API, however, we can expect data to follow strict guidelines. We'll be focusing on removing unnecessary data and changing Twitter data from the JSON we receive to a CSV that we can open in Excel or Google Sheets. Shifting Twitter data from JSON to CSV can turn a gigabyte of data into a megabyte of useful data (a 1000-fold) difference. ## Turn JSON into CSV ```python # import relevant dictionaries import json import requests import pandas as pd # load JSON from website r = requests.get('https://raw.githubusercontent.com/kmcelwee/fsi-web-scraping-seminar/main/data/dog_feelings-tweets.json') all_tweets = json.loads(r.text) # Cycle through the tweets in that JSON and collect the information we care about csv_dict = [] for tweet in all_tweets: csv_dict.append({ 'timestamp': tweet['created_at'], 'id': tweet['id'], 'text': tweet['full_text'], 'favorite_count': tweet['favorite_count'], 'retweet_count': tweet['retweet_count'], 'hashtags': ';'.join([h['text'] for h in tweet['entities']['hashtags']]) }) # Turn that list of dictionaries into a dataframe and save as a CSV df = pd.DataFrame(csv_dict) df.to_csv('dog_feelings-tweets.csv', index=False) ``` Here is a script that turns the JSON gathered from @dog_feelings tweets and turns it into a CSV with the most basic features like `timestamp` and `favorite_count`. A good way to build a CSV is to create a list of dictionaries, as shown above. This would look something like: ``` [ { 'timestamp': 'Wed May 10 03:16:19 +0000 2017', 'id': 862144241782444038, 'text': \"good. night. don't let. the bed bugs. bamboozle\", 'favorite_count': 1856, 'retweet_count': 376, 'hashtags': '' }, { 'timestamp': 'Wed May 10 00:44:59 +0000 2017', 'id': 862106154519977984, 'text': 'a thing. to remember: good things. are good. BUT. bad things. are not good. i think', 'favorite_count': 1562, 'retweet_count': 291, 'hashtags': '' } ... ] ``` This would create the following data frame: |timestamp |id |text |favorite_count|retweet_count|hashtags|------------------------------|------------------|-----------------------------------------------------------------------------------|--------------|-------------|--------|Wed May 10 03:16:19 +0000 2017|862144241782444038|good. night. don't let. the bed bugs. bamboozle |1856 |376 | |Wed May 10 00:44:59 +0000 2017|862106154519977984|a thing. to remember: good things. are good. BUT. bad things. are not good. i think|1562 |291 | Notice that each dictionary has the same keys. These are the column names. And then the associated value with each key is that column's value for the given row. Getting your head around this structure is fundamental, so if you don't fully understand, make sure to not just skim over this. The code above can be readily reused, as long as you can properly find the path to the tweet information that you need. ## Data validation Data validation is an important part of data cleaning, but it can get complicated. Fundamentally, you want to make sure that the assumptions that you have about your data set are actually true. The Twitter API is very consistent, so you shouldn't encounter too many problems, but consider using [python's `assert` statement](https://stackoverflow.com/questions/5142418/what-is-the-use-of-assert-in-python)) to double check your assumptions about how the data is structured. [I've outlined some hurdles](https://cdh.princeton.edu/updates/2021/03/19/mistakes-avoid-when-using-twitter-data-first-time/) I encountered when working with Twitter data and how I checked for them, but chances are, you won't encounter most of them if you keep to small data sets. ## Case Study 1: MTV Internship You're an intern at MTV and your boss wants to know if musicians active on Twitter get paid more money than those who aren't active on Twitter. 🎤 **What are some questions we should ask before pursuing this project?** View examples . | How will we consider artists that aren't on Twitter? | How will I get a financial data for all the artists? | Are different genders equally likely to be on Twitter? And what are the gender pay disparities in the music industry? | How do we define \"active on Twitter\"? One tweet a week? A month? | We'll have to manually relate artists to their Twitter accounts. How long will that take? | How might outliers distort our calculation? The entertainment industry follows the power law, meaning a small number of people make a majority of the money. If Beyonce, who doesn't tweet as much, commands 10x the money of Cardi B who tweets a lot, how that one data point skew our numbers? | . ## Tips * Data work is both unfulfilling and time consuming. Have a clear research question in mind before pursuing * Manually sorting data takes up more time than you think! Always run the calculations and weigh whether the research question you want to answer is worth the time that you will invest. ",
    "url": "/seminar-1/munge-json",
    "relUrl": "/seminar-1/munge-json"
  },"7": {
    "doc": "Python programming refresher",
    "title": "Python programming refresher",
    "content": "# Python programming refresher ## Tips on learning how to code The following are pieces of advice from programmers at the CDH and the library: * Google is a programmer's best friend. Any problem you have, there's a 99.9 percent chance another person has had that exact problem before. * There are so many free resources on the internet to help you. Use them! * This is just writing instructions. Programming isn't math. Don't count yourself out just because you aren't the best at mathematics. * The best way to become a coder is to code. Try things out. Mess up. * We're starting with python, but programming language really doesn't matter. The most important thing is to get started. * Don't expect to understand a topic the first time around. Be kind to yourself, this is a whole new way of thinking. * Be persistent. Coding takes time, and the solution is out there. You will figure it out if you keep working at it! * [Resist imposter syndrome.](https://adainitiative.org/continue-our-work/impostor-syndrome-training/) ## Python Review Exercise If you don't know how to proceed, try googling the solution. Prepare an answer before clicking the \"View Solution\" button. **Exercise 1:** Create a list of all integers between 1 and 10 and assign it to a variable. Print them out. View Solution **Exercise 2:** Write a sentence and store it as a variable. Write a function to return the word count of that sentence. View Solution **Exercise 3:** Make a dictionary that translates each letter of the alphabet into it's corresponding place in the alphabet (e.g. `\"a\" => 1`, `\"b\" => 2`) View Solution **Exercise 4:** Write a function that turns a word into a list of numbers that corresponds with their placement in the alphabet. Use the dictionary you created in the previous exercise. For example, `translate_word('princeton')` would return `[16, 18, 9, 14, 3, 5, 20, 15, 14]` View Solution **Exercise 5:** What are some errors you could encounter when writing the `translate_word` function? View Solution . | We would get an error if we fed 'Princeton' into the function because it contains an uppercase letter. (Try it out to see what kind of error you would get.) To ensure that the string is lowercase, use word = word.lower(). | This function expects a word. If we fed 'FSI Class of 2025!' into it, it would cause an error because the blank space character and the exclamation point isn't in our dictionary. Python's str.isalpha() function can tell us if any of the characters in a string aren't a letter. | . With these considerations in mind, we can rewrite this function. Code is never finished! We could keep rewriting many of these functions forever. Don't get too wrapped up in making your code \"perfect\". Write code that is clear, well-documented, and serves your ultimate research question. ",
    "url": "/seminar-1/python",
    "relUrl": "/seminar-1/python"
  },"8": {
    "doc": "Intro to Tweepy",
    "title": "Intro to Tweepy",
    "content": "# Introduction to Tweepy Let's log into the Twitter API and execute our first query! ## Twitter Advanced Search Before using the Twitter API, consider [Twitter Advanced Search](https://twitter.com/search-advanced?lang=en). You can often get a lot of answers before writing a line of code! Other forms of site-specific syntax can also be useful ## First API Login First, we need your authentication token. Log in to the developer portal. Create a new project under \"Projects & Apps\". Name your project `fsi-seminar` and answer that your purpose is to learn the Twitter API. After filling out the questionaire, you'll be met with the alphanumeric keys generated for you to use the Twitter API. Copy both the \"API Key\" and the \"API Secret Key\" into a new Colab notebook. ```python import tweepy # https://github.com/tweepy/tweepy consumer_key = \"\" # put your information here! consumer_secret = \"\" # put your information here! auth = tweepy.OAuthHandler(consumer_key, consumer_secret) api = tweepy.API(auth) # get tweet ID tweet = api.get_status(1412424266763603968) tweet.text ``` And there you go! You should see the text from a tweet from NASA. What do you notice about how the text is formatted? # Scrape a profile ```python def get_all_tweets(screen_name): alltweets = [] new_tweets = api.user_timeline(screen_name = screen_name, count=200) alltweets.extend(new_tweets) oldest = alltweets[-1].id - 1 # keep grabbing tweets until there are no tweets left to grab while len(new_tweets) > 0: new_tweets = api.user_timeline(screen_name = screen_name, count=200, max_id=oldest) alltweets.extend(new_tweets) oldest = alltweets[-1].id - 1 print(f\"...{len(alltweets)} tweets downloaded so far\") # transform the tweepy tweets into a 2D array that will populate the csv outtweets = [[tweet.id_str, tweet.created_at, tweet.text] for tweet in alltweets] # write the csv with open(f'new_{screen_name}_tweets.csv', 'w') as f: writer = csv.writer(f) writer.writerow([\"id\",\"created_at\",\"text\"]) writer.writerows(outtweets) get_all_tweets('SCREEN_NAME') ``` What do you notice about how this CSV was created differently than other code we've looked at? Toggle to see some answers: . | Because we're using tweepy, we can get to the text and timestamp information straight from what tweepy calls a Tweet object. You can't do this with JSON, but tweepy already parsed the most important features and making them easily accessible. | We're using python's csv library instead of Pandas to write the CSV. If you're not analyzing the data and just want to make a CSV, using this library can be useful. | . # Get profile information ```python user = api.get_user('SCREEN_NAME') ``` 🪐 **Exercise: Get the location of NASA through their Twitter profile.** View Solution # Collecting tweets by hashtag Read more about the search parameters [here](https://docs.tweepy.org/en/v3.5.0/api.html#help-methods). ```python tweet_list = api.search('#pride') for tweet in tweet_list: print(tweet.text) ``` You can read more about how these search strings can be constructed [here](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query). But tweepy isn't very reliable with complex searches. By default this command will return ten or so tweets, but by setting the `count` parameter, you can gather more. ",
    "url": "/seminar-1/tweepy",
    "relUrl": "/seminar-1/tweepy"
  }
}
